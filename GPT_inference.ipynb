{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "W tym notatniku będziemy mogli wytrenować lub korzystać z już wytrenowanego modelu.\n",
    "Tutaj model jest zbudowany z warstw wbudowanych w PyTorch, aby przyspieszyć jego trening.\n",
    "\n",
    "Po dokładny opis krok-po-kroku jak ten model powstał, odsyłam do notatnika `GPT.ipynb`\n"
   ],
   "id": "d546bea1f6fbb7af"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T22:11:45.594716Z",
     "start_time": "2024-12-15T22:11:45.584752Z"
    }
   },
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "context_size = 192\n",
    "train_iterations = 5_000\n",
    "learning_rate = 5e-4 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # wyćwiczenie tego modelu na cpu jest niemalże niemożliwe, inferencja jeszcze \"ujdzie\"\n",
    "embedding_size = 192\n",
    "amount_of_heads = 6\n",
    "amount_of_blocks = 4\n",
    "dropout = 0.1  # prawdopodobieństwo dropoutu\n",
    "MODEL_PATH = \"GPT_model.pth\"\n",
    "MODEL_LINK = f\"https://drive.google.com/uc?id=1ZQPHKPPda6mXOwuBaJZfcu142X1tYiQ-&export=download\"\n",
    "rng = torch.Generator()\n",
    "rng.manual_seed(42)\n",
    "\n",
    "# downloading the dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x73ce5eb6efd0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:11:47.630991Z",
     "start_time": "2024-12-15T22:11:45.703866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inference_or_train = input(\"Do you want to inference (I) already trained model or to train (T) a model?\")\n",
    "if inference_or_train.upper() == 'I':\n",
    "    inference_or_train = 'I'\n",
    "elif inference_or_train.upper() == 'T':\n",
    "    temp = input(\"Do you want to train new (N) model from scratch or an already existing (E) model?\")\n",
    "    if temp.upper() == 'N':\n",
    "        inference_or_train = \"TN\"\n",
    "    elif temp.upper() == \"E\":\n",
    "        inference_or_train = \"TE\"\n",
    "    else: raise ValueError(f' choice \"{temp}\" not recognized')\n",
    "else: raise ValueError(f' choice \"{inference_or_train}\" not recognized')\n",
    "\n",
    "dataset_link = \"https://drive.google.com/uc?id=1TQjhbN1jrQx7eMgySFkMfwahh7IZy2a8\"\n",
    "dataset_path = \"data/Shakespeare.txt\"\n",
    "if not os.path.isfile(dataset_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(dataset_link, dataset_path)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Dataset is already downloaded\")\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    whole_text = f.read() "
   ],
   "id": "1f55a81b3a6f844f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already downloaded\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:11:47.712048Z",
     "start_time": "2024-12-15T22:11:47.678421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vocabulary generation and vocab-to-token encodings\n",
    "def get_random_batch(split: str, batch_size:int, block_size: int, my_rng: torch.Generator) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns a random batch of data - tensor of shape (batch_size, block_size). \n",
    "    :param split: can be either \"train\" or \"test\". When train the train dataset is used\n",
    "    :returns: A tuple of two tensors - first with training data and second with labels/targets\n",
    "    \"\"\"\n",
    "    if split == \"train\": my_data = train_data\n",
    "    elif split == \"test\": my_data = test_data\n",
    "    else: raise ValueError(f\"Expected either `train` or `test` for the split argument, got {split} instead\")\n",
    "    idx = torch.randint(len(my_data) - block_size, size=(batch_size,), generator=my_rng)\n",
    "    # randint jest end-exclusive, dlatego nie trzeba modyfikować indeksów mimo że target jest i+1\n",
    "    x = torch.stack([data[i: i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "# train and evaluate model\n",
    "@torch.no_grad()\n",
    "def evaluate_model(m, split: str, n_samples: int) -> float:\n",
    "    xb, yb = get_random_batch(split, n_samples, context_size, rng)\n",
    "    m.eval()\n",
    "    logits = m(xb)\n",
    "    logits = logits.view(batch_size * context_size, vocab_size)\n",
    "    yb = yb.view(batch_size * context_size)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "    m.train()\n",
    "    return loss.item()\n",
    "\n",
    "def train_model(m, n_iter, context_size=context_size):\n",
    "    optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "    for i in range(n_iter):\n",
    "        xb, yb = get_random_batch(\"train\", batch_size, context_size, rng)\n",
    "        logits = m(xb)\n",
    "        # musimy nieco zmienić kształt, ponieważ funkcja cross_entropy oczekuje kształtu (Batch, Channels) dla Inputu oraz (Batch,) dla Targetu\n",
    "        # obecnie nasz input ma kształt (Batch, context_size, embedding_dim) a target (Batch, context_size). Musimy złączyć wymiary Batch oraz context w jeden\n",
    "        logits = logits.view(batch_size * context_size, vocab_size)\n",
    "        yb = yb.view(batch_size * context_size)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i  % (n_iter // 10) == 0:\n",
    "            train_loss = evaluate_model(m, \"train\", batch_size)\n",
    "            test_loss = evaluate_model(m, \"test\", batch_size)\n",
    "            print(f\"Iteration {i + 1} train loss: {train_loss:.3f} | test loss: {test_loss:.3f}\")\n",
    "        elif i == n_iter - 1:\n",
    "            train_loss = evaluate_model(m, \"train\", batch_size)\n",
    "            test_loss = evaluate_model(m, \"test\", batch_size)\n",
    "            print(f\"Final iteration train loss: {train_loss:.3f} | test loss: {test_loss:.3f}\")\n",
    "            \n",
    "from math import sqrt\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_size, head_size, context_size):\n",
    "        super().__init__()\n",
    "        self.sqrt_head_size = sqrt(head_size)\n",
    "        self.keys = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.queries = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.values = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        # wykorzystujemy register_buffer, aby jednocześnie zapisać ten Tensor w modelu i wykluczyć go z optymalizacji podczas treningu\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context_size, context_size))) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        k = self.keys(x)\n",
    "        q = self.queries(x)\n",
    "        wei = q @ k.transpose(-1, -2)\n",
    "        # przed wykonaniem softmaxu skalujemy dzieląc przez pierwiastek z head_size\n",
    "        wei = wei / self.sqrt_head_size\n",
    "        wei = wei.masked_fill(self.tril == 0, float(\"-inf\"))  \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.values(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "        \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, embedding_size:int, head_size: int, context_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(embedding_size, head_size, context_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # łączymy rezultaty w ostatnim wymiarze. Tworząc wiele głów, na przykład 4, zmniejszamy proporcjonalnie wymiar\n",
    "        # każdej z nich. Wtedy łącząc ich rezultaty w ostatnim wymiarze uzyskamy taki sam wymiar jak przy pojedyńczej Head\n",
    "        return torch.cat([single_head(x) for single_head in self.heads], dim=-1)\n",
    "            \n",
    "def try_model_generation(model_class, n_embd = 32, train_iterations=2500, context_size=7):\n",
    "    model_instance = model_class(vocab_size=vocab_size, n_embd=n_embd, head_size=18)\n",
    "    model_instance = model_instance.to(device)\n",
    "    xb, _ = get_random_batch(\"train\", batch_size, context_size, rng)\n",
    "    xb = xb.to(device)\n",
    "    train_model(model_instance, train_iterations, context_size=context_size)\n",
    "    generated_output = model_instance.generate(xb, 500)[0].cpu()\n",
    "    print(2 * \"\\n\")\n",
    "    print(f\"Generated 500 new tokens, output shape: {generated_output.shape}\")\n",
    "    print(20*\"-\" + \"\\n\" + f\"Generated text: {decode(generated_output.tolist())}\")   \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n:int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n, n * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n * 4, n)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, context_size):\n",
    "        super().__init__()\n",
    "        if n_embd % n_head != 0:\n",
    "            error_msg = (f\"Class Block expected n_embd to be divisible by n_head, but got {n_embd = } and {n_head = }\"\n",
    "                         f\"which leaves remainder of {n_embd % n_head}.\")\n",
    "            raise ValueError(error_msg)\n",
    "        single_head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadedAttention(n_embd, single_head_size, context_size, num_heads=n_head)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.layer_norm1 = nn.LayerNorm([n_embd])\n",
    "        self.layer_norm2 = nn.LayerNorm([n_embd])\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop(self.sa(self.layer_norm1(x)))  # Res connection: x + F(x)\n",
    "        x = x + self.drop(self.ffwd(self.layer_norm2(x)))  # Res connection: x + F(x)\n",
    "        return x\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, embedding_size)\n",
    "        blocks_list = [Block(embedding_size, amount_of_heads, context_size) for _ in range(amount_of_blocks)]\n",
    "        blocks_list.append(nn.LayerNorm([embedding_size]))\n",
    "        self.blocks = nn.Sequential(*blocks_list)\n",
    "        self.model_head = nn.Linear(embedding_size, vocab_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_indices):\n",
    "        B, T = input_indices.shape\n",
    "        token_emb = self.token_embedding_table(input_indices) # B, T, C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T, C\n",
    "        # automatyczny broadcasting pozwala dodać tensory (B,T,C) i (T,C)\n",
    "        x = token_emb + pos_emb\n",
    "        # dodajemy dropout po policzeniu sumy token embeddings i positional embeddings\n",
    "        x = self.drop(x)\n",
    "        # przeprowadzamy self-attention\n",
    "        x = self.blocks(x)\n",
    "        # dodajemy dropout przed wejściem do ostatniej warstwy Linear\n",
    "        x = self.drop(x)\n",
    "        # zamieniamy embeddingi na logity\n",
    "        logits = self.model_head(x)\n",
    "        return logits\n",
    "         \n",
    "    def generate(self, current_context: torch.Tensor, max_new_tokens: int):\n",
    "        \"\"\"\n",
    "        Metoda przyjmuje kontekst i na jego podstawie generuje `max_new_tokens` nowych tokenów.\n",
    "        :param current_context: Tensor o wymiarach (Batch, czas) gdzie 'czas' to kolejne znaki kontekstu\n",
    "        :return: Tensor o wymiarach (Batch, czas + max_new_tokens) zawierający podany kontekst, a do niego \"doklejoną\"\n",
    "            wygenerowaną treść\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # korzystamy z tabeli Embeddingów, więc musimy podać co najwyżej ostatnie context_size znaków\n",
    "            logits = self(current_context[:, -context_size:])\n",
    "            logits = logits[:, -1, :]  \n",
    "            probabilities = F.softmax(logits, dim=-1)  \n",
    "            idx_next = torch.multinomial(probabilities, num_samples=1)\n",
    "            current_context = torch.cat((current_context, idx_next), dim=1) # (Batch, czas + 1)\n",
    "        return current_context\n",
    "    \n",
    "def generate_using_model(m: BigramModel, max_new_tokens):\n",
    "    xb, _ = get_random_batch(\"test\", 2, context_size, rng)\n",
    "    generated_output = m.generate(xb, max_new_tokens)[0].cpu()\n",
    "    decoded_output = decode(generated_output.tolist())\n",
    "    # print(f\"Model was given context:\\n{decode((xb.cpu()).tolist())}\")\n",
    "    print(f\"\\n\\nModel generated:\\n{decoded_output}\")\n",
    "    "
   ],
   "id": "e6f00658a64b177d",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:12:19.568229Z",
     "start_time": "2024-12-15T22:11:47.738585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = sorted(list(set(whole_text)))  # set zapewnia unikalność znaków, lista daje się posortować\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab: {\"\".join(vocab)}\")\n",
    "print(f\"Vocab len: {len(vocab)}\")\n",
    "\n",
    "stoi = {char: i for i, char in enumerate(vocab)}\n",
    "itos = {i: char for i, char in enumerate(vocab)}\n",
    "encode = lambda s: [stoi[c] for c in s]  # zamienia string na listę liczb\n",
    "decode = lambda l: \"\".join(itos[i] for i in l)\n",
    "\n",
    "data = torch.tensor(encode(whole_text))\n",
    "data = data.to(device)\n",
    "\n",
    "train_to_all_ratio = 0.85\n",
    "n = int(train_to_all_ratio * data.numel())\n",
    "train_data, test_data = data[:n], data[n:]\n",
    "print(f\"{len(train_data) = }, {len(test_data) = }\")\n",
    "\n",
    "if inference_or_train[0] == \"T\":\n",
    "    if inference_or_train[1] == \"E\": \n",
    "        model = None\n",
    "        raise NotImplementedError(\"Dotrenowywanie modeli jeszcze nie jest zaimplementowane, upsss\")\n",
    "    elif inference_or_train[1] == \"N\":\n",
    "        model = BigramModel().to(device)\n",
    "        model.train()\n",
    "        train_model(model, train_iterations, context_size=context_size)\n",
    "        print(\"saving model...\", end=\"\")\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(\"model saved\")\n",
    "        \n",
    "elif inference_or_train[0] == \"I\":\n",
    "    model = BigramModel().to(device)\n",
    "    should_download = input(\"Do you want to download (Y) the model, or is it already downloaded?\")\n",
    "    if should_download.upper() == \"Y\":\n",
    "        gdown.download(MODEL_LINK, MODEL_PATH)\n",
    "    else:\n",
    "        print(\"Not downloading the model then, accessing locally stored model...\", end=\"\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, weights_only=True, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"model loaded\")\n",
    "    how_many_tokens = int(input(\"How many tokens do you wish to generate during inference?\"))\n",
    "    generate_using_model(model, how_many_tokens)\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ],
   "id": "96281dae5b09bd87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab len: 65\n",
      "len(train_data) = 948084, len(test_data) = 167310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZQPHKPPda6mXOwuBaJZfcu142X1tYiQ-&export=download\n",
      "To: /home/uxert/projekty/ProjektLLM/GPT_model.pth\n",
      "100%|██████████| 10.4M/10.4M [00:00<00:00, 18.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "\n",
      "\n",
      "Model generated:\n",
      " less expected: he replied,\n",
      "It was a bare petition of a state\n",
      "To one whom they had punish'd.\n",
      "\n",
      "MENENIUS:\n",
      "Very well:\n",
      "Could he say less?\n",
      "\n",
      "COMINIUS:\n",
      "I offer'd to awaken his regard\n",
      "For's private from what you unto you,\n",
      "Which wrings your care out oned? Mareius Warwick, joy\n",
      "That more shadow. What dreams?\n",
      "\n",
      "STANLEY:\n",
      "No, no provost to the popular, sir, though; go to.\n",
      "\n",
      "KING RICHARD III:\n",
      "Peace! and I see when over striction: if not acquaintance.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "My love! thy minds I keep me Paris.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Heaven, and be, perform it be in God's head!\n",
      "\n",
      "WARWICK:\n",
      "Say, my lord, I Somerset, seek my lodge!\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "What is yourself?\n",
      "\n",
      "NORFOLK:\n",
      "Well, Signor, by your Paulina, in heaven,\n",
      "Here an oad-faced night glory's name.\n",
      "\n",
      "HASTINGS:\n",
      "Be gone, for me!\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "No, never and we will live our groans,\n",
      "To medicine.\n",
      "\n",
      "KING RICHARD III:\n",
      "Tyrantly; and not trickly on thy hands,\n",
      "Both and defend mack the lamphal to see, call your hope:\n",
      "Therefore was king sovereignly in part.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Thy poor cloy, hear me lie: the gods by arms?\n",
      "\n",
      "LADY GREY:\n",
      "What, I have thee there?\n",
      "\n",
      "KING HENRY VI:\n",
      "My Lord of Hurchbishop, by a virtuous still.\n",
      "\n",
      "DORSET:\n",
      "But be no more to thy spreadure is copy.\n",
      "\n",
      "JULIET:\n",
      "O love, love! How now! If this arm!\n",
      "Sweet praise, that you kneel what you that slast thee?\n",
      "\n",
      "GLOUCESTER:\n",
      "To let him flighter thee gone? Farewell! how; let me report\n",
      "That I fear, thought found i' the county of gates.\n",
      "We may call of grace: in your tenderness to\n",
      "Ere swelcome; but thou'rt it ill, and so greater; but\n",
      "I did not the by the York: what is much\n",
      "Jesu an one Duke of Norfolk, comes from this death\n",
      "Have patiences, what my mightle mother\n",
      "To the manner break that she's fleece will her market:\n",
      "That says think it with dogs-stainly on me may\n",
      "Lamentable made to seek the king way.\n",
      "\n",
      "JULIET:\n",
      "No, methinks by the faults be many from misery\n",
      "Of which army thy brother fellows our head,\n",
      "But not slaughter'd in a prayer of thing.\n",
      "From justice, if it were to post him.\n",
      "And Romeo here be still, will I home:\n",
      "Vile him running to thy deliver'd before it hath been,\n",
      "To like a perceive a tribune and pount truth,\n",
      "That further, and thou didst dear it, to do me glory\n",
      "Cut me to justice: of an asswer of \n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:12:19.593688Z",
     "start_time": "2024-12-15T22:12:19.589928Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "373898c9adab92cc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
